# -*- coding: utf-8 -*-
"""DL_Assignment#1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fPxdF31bOPF_-856X0P3ESaoVfffByKs
"""

!git clone https://github.com/Mario-Hunter/deep_learning.git

cd deep_learning/

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib

import matplotlib.pyplot as plt
from scipy.stats import skew
from scipy.stats.stats import pearsonr
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.regularizers import l1, l2
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook
# %matplotlib inline

#Reading training data
train = pd.read_csv("assignment1_housing_train.csv")
#Reading testing data
test = pd.read_csv("assignment1_housing_test.csv")
all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],
                      test.loc[:,'MSSubClass':'SaleCondition']))

print(train.shape,test.shape)

#log transform the target y in training data - by reference inside all
train["SalePrice"] = np.log1p(train["SalePrice"])

#log transform skewed numeric features:

# Get Numerical Fields
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index 

skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc
skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns
skewed_feats = skewed_feats.index # Get Skewed Columns indices

#log scale skewed columns
# normalize the skewed distribution for better regression
all_data[skewed_feats] = np.log1p(all_data[skewed_feats])

# Create Dummy variables for the categorical features 
all_data = pd.get_dummies(all_data) 

# Replace the numeric missing values (NaN's) with the mean of their respective columns
all_data = all_data.fillna(all_data.mean())

#splitting the data to training & testing
X_train = all_data[:train.shape[0]]
X_test = all_data[train.shape[0]:]
y = train.SalePrice

# Standardize features by removing the mean and scaling to unit variance
# z = (x - u) / s
X_train = StandardScaler().fit_transform(X_train)

#split training data into training & validation, default splitting is 25% validation
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from keras import backend as K

optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
losses = ['mean_absolute_percentage_error','mean_squared_error','mean_squared_logarithmic_error']

def build_model(loss="mean_squared_error",optimizer='Adam',activation='linear'):
  model = keras.Sequential([layers.Dense(1, activation=activation, input_shape=[X_tr.shape[1]])])
  model.compile(loss = loss, optimizer = optimizer)
  #model.summary()
  hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500,verbose=0)
  return model,hist

def visualize(hist):
  # Get training and test loss histories
  training_loss = hist.history['loss']
  val_loss = hist.history['val_loss']
  # Create count of the number of epochs
  epoch_count = range(1, len(training_loss) + 1)
  # Visualize loss history
  plt.figure()
  plt.plot(epoch_count, training_loss, 'r--')
  plt.plot(epoch_count, val_loss, 'b-')
  plt.legend(['Training Loss', 'Val Loss'])
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.show()

for op in optimizers:
  for loss in losses:
    print("Using Optimizer:",op,"with loss function:",loss)
    model,hist = build_model(loss,op)
    visualize(hist)

print(model.predict(X_test))

"""**Logistic Regression**"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop
from tensorflow.keras.initializers import RandomNormal, RandomUniform
from tensorflow.keras.losses import BinaryCrossentropy, CategoricalHinge, MeanSquaredError
from tensorflow.keras import regularizers

dataset = pd.read_csv("heart.csv")

X = dataset.iloc[:, 0:13].values
y = dataset.iloc[:, 13].values
y = y.reshape(len(y),1)

def boxPlotDraw(dataList):
  fig = plt.figure(1, figsize=(12, 15))
  ax = fig.add_subplot(111)
  bp = ax.boxplot(dataList)
  plt.show()
for col in range(X.shape[1]):
  print(col)
  boxPlotDraw(X[col].tolist())
print(X.shape,y.shape)

## TODO START:: Data Pre-Processing
from sklearn import preprocessing
normalization = preprocessing.StandardScaler()
X = normalization.fit_transform(X)



## TODO End:: Data Pre-Processing

# Get Training Data
train_X, temporary_X, train_y, temporary_y = train_test_split(X, y, train_size=0.75, random_state=0)

# Get Validation & Testing Data
val_X, test_X, val_y, test_y = train_test_split(temporary_X, temporary_y, train_size=0.5, random_state=0)

optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
losses = ['binary_crossentropy','mean_squared_error','mean_squared_logarithmic_error']

## TODO START:: Model Definition, Writing the Model using Tensorflow.Keras is a must
def build_model(loss="binary_crossentropy",optimizer='Adam',activation='sigmoid'):
  output_dim = 1
  model = Sequential() 
  model.add(Dense(output_dim, input_dim=X.shape[1], activation=activation)) 
  model.compile(loss=loss, metrics=['accuracy'], optimizer=optimizer)
  hist = model.fit(train_X, train_y, verbose=0, validation_data=(val_X, val_y), batch_size=16, epochs=500)
  return model,hist
## TODO END:: Model Definition

for op in optimizers:
  for loss in losses:
    print("Using Optimizer:",op,"with loss function:",loss)
    model,hist = build_model(loss,op)
    ## TODO Try Different losses & optimizers here
    score, accuracy = model.evaluate(test_X, test_y, batch_size=16, verbose=0)
    print("Test fraction correct (NN-Score) = {:.2f}".format(score))
    print("Test fraction correct (NN-Accuracy) = {:.2f}".format(accuracy))
    visualize(hist)



